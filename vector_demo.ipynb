{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjjeapFAW0E4"
      },
      "outputs": [],
      "source": [
        "!pip install chromadb\n",
        "!pip install PyPDF2\n",
        "!pip install langchain\n",
        "!pip install sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import chromadb\n",
        "import PyPDF2\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings"
      ],
      "metadata": {
        "id": "kouek3gDW2Yf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = chromadb.Client() # this is similar to the create database\n",
        "client.list_collections()\n",
        "collection = client.create_collection(name = \"sample_collection\",metadata={\"hnsw:space\": \"cosine\"})\n",
        "file_path ='/content/drive/MyDrive/LLM_data/snowflake_container.pdf'"
      ],
      "metadata": {
        "id": "5xbIOvLaW60x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client.list_collections()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeSixevimAL_",
        "outputId": "6d06f972-ca35-4b62-b407-116e648a78e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Collection(name=sample_collection)]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_file = open(file_path, 'rb')\n",
        "pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
        "text = \"\"\n",
        "for page_num in range( len(pdf_reader.pages)):\n",
        "    text += pdf_reader.pages[page_num].extract_text()\n",
        "pdf_file.close()"
      ],
      "metadata": {
        "id": "F0Ckq7rLW77I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the text into chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100,separators=\"\\n\\n \")\n",
        "chunks = text_splitter.split_text(text)"
      ],
      "metadata": {
        "id": "LrUAbBreXBYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents_list = []\n",
        "ids_list = []\n",
        "for i, chunk in enumerate(chunks):\n",
        "    documents_list.append(chunk)\n",
        "    ids_list.append(f\"snw_{i}\")"
      ],
      "metadata": {
        "id": "WRZ039mKXDXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# insert into table\n",
        "collection.add(\n",
        "    documents=documents_list,\n",
        "    ids=ids_list\n",
        ")"
      ],
      "metadata": {
        "id": "1OL2DQRHXIAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# select *\n",
        "collection.get([],)"
      ],
      "metadata": {
        "id": "bedkWM3GXLd_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ed56128-efce-475a-932a-9db696a3d402"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ids': ['snw_0',\n",
              "  'snw_1',\n",
              "  'snw_10',\n",
              "  'snw_11',\n",
              "  'snw_12',\n",
              "  'snw_13',\n",
              "  'snw_14',\n",
              "  'snw_15',\n",
              "  'snw_16',\n",
              "  'snw_17',\n",
              "  'snw_18',\n",
              "  'snw_19',\n",
              "  'snw_2',\n",
              "  'snw_20',\n",
              "  'snw_21',\n",
              "  'snw_22',\n",
              "  'snw_23',\n",
              "  'snw_3',\n",
              "  'snw_4',\n",
              "  'snw_5',\n",
              "  'snw_6',\n",
              "  'snw_7',\n",
              "  'snw_8',\n",
              "  'snw_9'],\n",
              " 'embeddings': None,\n",
              " 'metadatas': [None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None],\n",
              " 'documents': ['PDFm yURL converts web pages and ev en full websites to PDF easily and quickly .\\nSnowpark Container Services — A\\nTech Primer\\nCaleb Baechtold·Follow\\nPublished inSnowflake·10 min read·Jul 7, 2023\\n91 2\\nUpdated 12/20/2023\\nIntroduction\\nAt our annual Summit 2023 user conference the last week of June, Snowflake\\nannounced a new product feature, Snowpark Container Services. Snowpark\\nContainer Services is a fully managed container offering that allows you to\\neasily deploy, manage, and scale containerized services, jobs, and functions,\\nall within the security and governance boundaries of Snowflake, and\\nrequiring zero data movement. As a fully managed service, Snowpark\\nContainer Services comes with Snowflake’s native security, RBAC support,\\nand built-in configuration and operational best-practices.\\nSnowpark Container Services are fully integrated with both Snowflake\\nfeatures and third-party tools, such as Snowflake Virtual Warehouses and',\n",
              "  'features and third-party tools, such as Snowflake Virtual Warehouses and\\nDocker, allowing teams to focus on building data applications, and not\\nbuilding or managing infrastructure. Just like all things Snowflake, this\\nmanaged service allows you to run and scale your container workloads\\nacross regions and clouds without the additional complexity of managing a\\ncontrol plane, worker nodes, and also while having quick and easy access to\\nyour Snowflake data.\\nSearch Sign up Sign in Write\\nSign up to discover human stories that\\ndeepen your understanding of the world.Free\\nDistraction-free reading. No ads.\\nOrganize your knowledge with lists and\\nhighlights.\\nTell your story. Find your audience.\\nSign up for freeMembership\\nAccess the best member-only stories.\\nSupport independent authors.\\nListen to audio narrations.\\nRead offline.\\nJoin the Partner Program and earn for\\nyour writing.\\nTry for $5/monthPDFm yURL converts web pages and ev en full websites to PDF easily and quickly .',\n",
              "  'compute pools and give you the flexibility to run jobs that could not\\notherwise be supported in a SQL or Snowpark Python/Java/Javascript task.\\nExample: GPU-accelerated model training\\nSnowpark Container Services can run on GPU-powered compute pools,\\nincluding both A10 and A100 NVIDIA GPUs, which enable a variety of ML\\nand LLM-specific workloads. In the context of GPU-enabled machine\\nlearning, model training is a great example of a Job service. This is a script to\\nperform model training using GPU-acceleration, CUDA drivers, and\\ncompatible open source libraries. It is triggered for execution (automatically,\\nthrough a Snowflake task, or manually via a user EXECUTE SERVICE\\nstatement), and runs to completion.PDFm yURL converts web pages and ev en full websites to PDF easily and quickly .\\nServices\\nServices in Snowpark Container Services are exactly that- long-running\\nservices which exist to provide some sort of accessible functionality,',\n",
              "  'services which exist to provide some sort of accessible functionality,\\npotentially through an externally accessible UI, programmatic access, or to\\nperform some sort of other persistently running task. This model mirrors a\\ncommon cloud microservices architecture, and supports hosting of services\\nsuch as Jupyter notebooks, REST APIs, or even a Kafka connect cluster that\\nreaches outside of Snowflake to stream/ingest data into Snowflake.\\nExamples: Hosted Jupyter notebooks, REST APIs, Kafka Connect\\nThere are two sub-models within the notion of long-running Services as\\nSnowpark Container Services. These are (1) services that users reach into\\nfrom outside of snowflake (e.g. a User Interface such as Hosted Jupyter\\nNotebooks), and (2) services that reach out of Snowflake (e.g. Kafka\\nConnect). Of course, certain services might fit both of these models. In\\neither case, these are persistent services that are “up and running”, and',\n",
              "  'either case, these are persistent services that are “up and running”, and\\npotentially made accessible through public endpoints. These long-running\\nservices additionally support both UI and programmatic authentication and\\naccess.\\nAs an example, to host Jupyterlab in Snowflake running as a Snowpark\\nContainer Service, you need the following specification yaml:\\nspec:\\n  container:\\n  - name: jupyterlab\\n    image: <org>-<account>.registry.snowflakecomputing.com/<my-db>/<my-schema>/<\\n    env:\\n      JUPYTER_ENABLE_LAB: \"yes\"\\n      JUPYTER_TOKEN: \"docker\"\\n  endpoints:\\n  - name: \"ui\"\\n    port: 8888\\n    public: true\\nThe service is then created via a simple SQL statement:PDFm yURL converts web pages and ev en full websites to PDF easily and quickly .\\ncreate compute pool if not exists JUPYTERLAB_STANDARD_2_CPU\\n    min_nodes = 1\\n    max_nodes = 1\\n    instance_family = standard_2\\n    auto_resume = true;\\ncreate service jupyterlab \\n    min_instances=1 \\n    max_instances=1',\n",
              "  'auto_resume = true;\\ncreate service jupyterlab \\n    min_instances=1 \\n    max_instances=1 \\n    compute_pool=JUPYTERLAB_STANDARD_2_CPU \\n    spec=@<MY-DB>.<MY-SCHEMA>.service_yaml/jupyterlab_spec.yaml;\\nFrom there, we have Jupyterlab running securely inside of Snowflake,\\naccessible to users via browser:\\nJupyterlab running as a Snowpark Container Service\\nService Functions\\nService Functions in Snowpark Container Services are services that take in\\ndata as an input parameter, for example in the context of table columns in aPDFm yURL converts web pages and ev en full websites to PDF easily and quickly .\\nSQL query, and run some custom processing against that data, returning a\\nresult. The framework for service functions follow the same structure as\\nSnowflake external functions in terms of data patterns, with of course the\\ndifference that with Snowpark Container Services, the data never actually\\nleaves Snowflake. A function-as-a-service requires two components: the',\n",
              "  'leaves Snowflake. A function-as-a-service requires two components: the\\nservice, which specifies the logic of how your function processes input data,\\nimplemented via a REST interface that responds to POST requests, and then\\na configured SQL function which references the specific service and\\nendpoint. For example, an open-source LLM can be loaded into a service on\\nGPU compute with a configured REST endpoint. A function can then be\\ndefined which specifies input string (such as a user prompt), calls the service\\nat the endpoint, and then returns the generated result. This can also support\\nrunning arbitrary functional code that may not be otherwise supported in\\nSnowflake using SQL or Snowpark User-Defined Functions (UDFs).\\nExample: OSS LLMs (e.g. Dolly, LLaMa, E5)\\nService Functions provide a mechanism to host arbitrary functional code to\\nprocess data in a Snowpark Container Service. These service functions use\\nthe Snowflake external function structures to expose named SQL functions',\n",
              "  'the Snowflake external function structures to expose named SQL functions\\nthat can be called in a query, table definition, and more. Importantly, even\\nthough service functions follow the external function ICD in terms of how\\nthe virtual warehouse structures and passes data to the Snowpark Container\\nService, this all occurs within the Snowflake network boundary (i.e. no data\\nmovement- maintaining complete control, governance, and security of your\\ndata). Examples of this are hosting open-source LLMs to accept user prompts\\nabout your Snowflake data, as well as arbitrary data processing code (in any\\nlanguage).\\nCheck out Reka running as a Native App inside of Snowpark Container\\nServices as an example.\\nCommunicating with Services\\nIn thinking about a full application stack running as a service(s) on\\nSnowpark Container Services, an obvious question is: how do IPDFm yURL converts web pages and ev en full websites to PDF easily and quickly .',\n",
              "  'communicate with different containers and/or services? Web applications\\nneed to be accessible by users (potentially over the internet); those same\\nweb UIs may need to communicate with REST API containers running inside\\nthe same service; functions may need to reach outside of Snowflake, too.\\nService communication is supported in three different ways: ingress\\n(reaching into a service from outside), egress (reaching outside of Snowflake\\nfrom inside a service), and inter-service communication (containers\\ncommunicating with each other without traversing the open internet).\\nIngress\\nServices running in Snowpark Container Services can optionally support\\ningress, that is, the ability for users, applications, and services from outside\\nof Snowflake reaching into and interacting with the service running inside of\\nSnowflake. This is done by optionally configuring endpoints (i.e. ports) as\\npublic in the service specification file. For example, if you want to run',\n",
              "  'public in the service specification file. For example, if you want to run\\nJupyter notebooks as a hosted service that can be accessed by users, you\\nsimply specify port 8888 as public: true in your specification file. This will\\ngenerate a publicly accessible URL for the corresponding endpoint which\\nusers can use to reach into the service, login, and connect to it. These public\\nendpoints can also be accessed programmatically through oauth tokens,\\nwhich supports hosting REST API services that can be accessed by other\\nservices running outside of Snowflake.\\nEgress\\nServices running in Snowpark Container Services can also reach out to other\\napplications, endpoints, or the public internet (egress outside of Snowflake).\\nThis is configurable via network policies and External Access Integrations,\\nproviding fine-grained security controls over what external resources\\nservices can communicate with.\\nInter-service Communication\\nIndividual endpoints (even that are non-public) in a service can be',\n",
              "  'Inter-service Communication\\nIndividual endpoints (even that are non-public) in a service can be\\ncommunicated with by other services (or between containers in the samePDFm yURL converts web pages and ev en full websites to PDF easily and quickly .\\nservice) using a generated DNS name and the corresponding port number\\nwithout having to traverse the public internet. When the service is created,\\nSnowflake generates a DNS name of the format:\\n<service-name>.<schema>.<db>.snowflakecomputing.internal\\nwhich can be used for inter-service communication. Note that this is a\\nstandardized DNS name, whereas public endpoints for ingress are randomly\\ngenerated and must be published; the result is that programmatic discovery\\nof services for interservice communication is not necessary, since service\\nendpoints will be exposed at known DNS locations.\\nPartner Applications\\nDuring the product keynote at Snowflake Summit 2023, we unveiled\\nSnowpark Container Services by showing more than 10 Snowflake partner',\n",
              "  'Snowpark Container Services by showing more than 10 Snowflake partner\\ntools running natively inside of Snowflake. For more info about our Design\\nand Launch partners, check out the official Snowflake blog announcement\\non Snowpark Container Services.\\nYou can also check out a number of these partner tools running in Snowpark\\nContainer Services on our YouTube channel:\\nHEX & NVIDIA\\nReka\\nKumo.ai\\nSAS\\nRelationalAI\\nWeights & Biases\\nAlteryxPDFm yURL converts web pages and ev en full websites to PDF easily and quickly .\\nDataiku\\nH2O\\nYou can always find more on the Snowflake Developers YouTube channel.\\nConclusion\\nWith Snowpark Container Services, Snowflake customers are now able to\\nhost and run arbitrary containerized data-intensive applications, colocated\\nwith their data, and utilizing a fully managed service.\\nTo get started with Snowpark Container Services, check out our Intro to\\nSnowpark Container Services Quickstart. For more information about',\n",
              "  'Try for $5/monthPDFm yURL converts web pages and ev en full websites to PDF easily and quickly .\\nHow are customers and partners using Snowpark Container Services today?\\nContainerized services on Snowflake open up the opportunity to host and\\nrun long-running services, like front-end web applications, all natively\\nwithin your Snowflake environment. Customers are now running GPU-\\nenabled machine learning and AI workloads, such as GPU-accelerated model\\ntraining and open-source Large Language Models (LLMs) as jobs and as\\nservice functions, including fine-tuning of these LLMs on your own\\nSnowflake data, without having to move the data to external compute\\ninfrastructure. Snowpark Container Services are an excellent path for\\ndeploying applications and services that are tightly coupled to the Data\\nCloud.\\nWith all of the excitement and buzz around Snowpark Container Services,\\nwe want to provide a detailed technical explainer that outlines the different',\n",
              "  'Snowpark Container Services Quickstart. For more information about\\nSnowpark Container Services, check out the documentation and contact\\nyour Snowflake account team or sales representative.\\nWatch the full announcement from the Opening Keynote at Snowflake\\nSummit 2023!\\n91 2\\nWritten by Caleb Baechtold\\n177 Followers·Writer for SnowflakeFollowSnowpark Snowflake Containers Application AIPDFm yURL converts web pages and ev en full websites to PDF easily and quickly .\\nML/AI Field CTO @ Snowflake. Mathematician, artist & data nerd. Alumnus of the Johns\\nHopkins University. @clbaechtold — Opinions my own\\nMore from Caleb Baechtold and Snowflake\\nCaleb BaechtoldinSnowflake\\nEnd-to-End MLOps with Snowpark\\nPython and MLFlow\\nUpdate February 14, 2023: The MLFlow API is\\nnow available via Snowpark/Anaconda, whic…\\n4 min read·Jan 9, 2023\\n--Somen SwaininSnowflake\\n9 Snowflake Tables- A briefing(as\\nof 2023)\\nIn this blog I would be discussing about\\nvarious Snowflake tables and also some of…',\n",
              "  \"of 2023)\\nIn this blog I would be discussing about\\nvarious Snowflake tables and also some of…\\n12 min read·Dec 19, 2023\\n-- 1\\nKeith SmithinSnowflake\\nSnowflake and Power BI: Best\\nPractices and Recent…\\nApproaches to maximize performance\\n10 min read·Dec 7, 2023\\n-- 1Caleb BaechtoldinSnowflake\\nOperationalizing Snowpark\\nPython: Part One\\nIn part one of this post, we will outline some of\\nthe unique challenges associated with…\\n14 min read·Nov 8, 2022\\n--PDFm yURL converts web pages and ev en full websites to PDF easily and quickly .\\nSee all from Caleb Baechtold See all from Snowflake\\nRecommended from Medium\\nSudhenduinSnowflake\\nSnowflake Snowpark Container\\nServices- Explain, please!\\nEverything you wanted to ask but couldn't\\xa0…..\\n7 min read·Jul 19, 2023\\n--Mauricio Rojas\\nIngest External Data Into\\nSnowflake with SnowPark and…\\nWith the introduction of external network\\naccess capabilities, Snowflake has taken dat…\\n5 min read·Oct 8, 2023\\n--\\nLists\\nGenerative AI Recommended\\nReading\",\n",
              "  '5 min read·Oct 8, 2023\\n--\\nLists\\nGenerative AI Recommended\\nReading\\n52 stories·604 savesWhat is ChatGPT?\\n9 stories·269 saves\\nThe New Chatbots: ChatGPT,\\nBard, and Beyond\\n12 stories·265 savesNatural Language Processing\\n1084 stories·556 saves\\nPDFm yURL converts web pages and ev en full websites to PDF easily and quickly .\\nSriganesh Palani\\nManage snowflake data loading\\nwith Python script\\nWe can integrate the data to snowflake in\\nmultiple ways, In this post will focus on…\\n2 min read·Dec 27, 2023\\n--KaarthikandavarinTowards AI\\nsnowChat: Leveraging OpenAI’s\\nGPT for SQL queries\\nInteract with your Snowflake database using\\nnatural language queries\\n·5 min read·Jul 31, 2023\\n--\\nJeremy Griffith\\nAI-Driven Insights with Snowflake\\nCortex and Streamlit\\nBridge the gap between BI and insights using\\nStreamlit and Snowflake Cortex\\n5 min read·Dec 7, 2023\\n-- 2Johan\\nSnowflake & CI/CD\\nMy case for CI/CD in Snowflake is very\\nspecific: every 30 days I register for a new…\\n·2 min read·Dec 2, 2023\\n-- 1',\n",
              "  'specific: every 30 days I register for a new…\\n·2 min read·Dec 2, 2023\\n-- 1\\nSee more recommendationsPDFm yURL converts web pages and ev en full websites to PDF easily and quickly .\\nHelp Status About Careers Blog Privacy Terms Text to speech Teams',\n",
              "  'we want to provide a detailed technical explainer that outlines the different\\nobjects and components that make up a Snowpark Container Service, as well\\nas outline the functional models that are supported and common patterns of\\nusage.\\nPDFm yURL converts web pages and ev en full websites to PDF easily and quickly .\\nSnowpark Container Services: Object Model\\nThe introduction of Snowpark Container Services on Snowflake includes the\\nincorporation of new object types and constructs to the Snowflake platform,\\nnamely: images, image registry, image repositories, compute pools,\\nspecification files, services, and jobs.\\nPDFm yURL converts web pages and ev en full websites to PDF easily and quickly .\\nImages\\nThe notion of an image in Snowpark Container Services is equivalent to that\\nof an OCI-compliant image; that is, an image is a file used to execute code in\\nan OCI-compliant container runtime such as Docker. Its build process and',\n",
              "  'an OCI-compliant container runtime such as Docker. Its build process and\\nexecution commands are specified in a file such as a Dockerfile, and images\\nare built into containers and then ultimately pushed to an image repository\\nfor execution in Snowpark Container Services.\\nImage Registry\\nImage registry in Snowflake is an OCIv2 compliant service for storing OCI-\\ncompliant container images. The image registry has a unique hostname\\nwhich allows OCI clients to access an image registry using REST API calls.\\nWithin a registry, there is a notion of image repositories which is a storage\\nunit within that image registry service.\\nImage Repositories\\nPDFm yURL converts web pages and ev en full websites to PDF easily and quickly .\\nImage repositories are the landing zone for built containers. In Snowflake,\\nan image repository is effectively a storage mechanism for built images\\nwhich is mapped inside of a Snowflake database and schema with a\\ncorresponding FQDN:',\n",
              "  'which is mapped inside of a Snowflake database and schema with a\\ncorresponding FQDN:\\nCREATE IMAGE REPOSITORY <db>.<schema>.<image_repo_name>;\\nThe corresponding tagged images in a Snowflake image repository are used\\nin the specification file for a service to define the source image of each\\ncontainer comprising the service:\\nimage: my-org-my-account.registry.snowflakecomputing.com/<db>/<schema>/<image_re\\nCompute Pools\\nA service in Snowpark Container Services needs compute resources to\\nactually run, and a compute pool is a collection of virtual machines or nodes\\non which those services run. Compute pools are created and managed\\nsimilarly to virtual warehouses on Snowflake, including\\nsuspension/resumption, but give you the flexibility to specify different\\nmachine types and min/max node counts to scale to on any one compute\\npool. Snowflake handles this auto-scaling of the compute pool based on the\\nconfigured node counts, the number of instances of a particular service',\n",
              "  'configured node counts, the number of instances of a particular service\\nrunning on a compute pool, and the number of different services running on\\nthat compute pool. For services with multiple instances running across one\\nor several nodes in a compute pool, Snowflake automatically load balances\\nincoming traffic to that service across instances.\\nThe Public Preview of Snowpark Container Services supports standard CPU,\\nhigh-memory, and GPU instance types in a variety of sizes. Like Snowflake\\nvirtual warehouses, compute pools are billed to the customer account on aPDFm yURL converts web pages and ev en full websites to PDF easily and quickly .\\nper-second basis, using a standard Snowflake credits/hour rate depending on\\nthe compute pool’s instance type, size, and number of nodes in the pool. For\\nmore detail about the exact specifications and sizes of node types available\\nto compute pools, contact your Snowflake representative.\\nSpecification Files',\n",
              "  'to compute pools, contact your Snowflake representative.\\nSpecification Files\\nSpecification files are YAML files that provide the definition for a service.\\nThe specification file defines the container(s) that comprise the service, the\\ncorresponding image(s) in the image registry that back them, and relevant\\nenvironment variables, endpoint definitions, volume mounts, and more. For\\nKubernetes users, Snowpark Container Services specification files are\\nsimilar to a Kubernetes deployment manifest. The specification file for a\\nparticular service is then uploaded to a Snowflake stage and then the\\ncorresponding staged file is referenced in the CREATE SERVICE statement.\\nServices\\nServices are the actual applications, container(s), etc. that run in Snowpark\\nContainer Services and are created from specification files to execute on\\ncompute pools. Users can perform standard CRUD operations on services,\\nand system functions allow users to retrieve information about the service',\n",
              "  'and system functions allow users to retrieve information about the service\\nruntime status, local logs from the container service, and more. Services\\nthemselves do not incur any cost, as Snowflake only bills customers for\\nactive compute pools and corresponding storage of images. Services and the\\ncorresponding containers that comprise them support three different design\\nmodels for deployment, which are described in detail in the subsequent\\nsection.\\nSnowpark Container Services Design Models\\nThe initial release of Snowpark Container Services supports three core\\nservice design models, on top of which complex data intensive applications\\ncan be built: jobs, functions, and services:PDFm yURL converts web pages and ev en full websites to PDF easily and quickly .\\nJobs are containerized tasks that are executed, perform some sort of\\noperation, and run to completion (e.g. GPU-accelerated machine learning\\nmodel training).\\nServices are long-running applications that are accessible via',\n",
              "  'model training).\\nServices are long-running applications that are accessible via\\ncombinations of internal and external endpoints (e.g. web UI, REST API,\\nKafka Connect cluster).\\nService Functions are callable bits of computation to which data is\\npassed, a response is produced and returned (e.g. prompting OSS LLMs).\\nImportantly, with service functions that are called on Snowflake data, the\\ndata never actually leaves Snowflake.\\nWe will look at each of these service models in more detail:\\nJobs\\nJobs in Snowpark Container Services are containerized tasks that are\\nexecuted, and run to completion over some bounded time domain. At\\ninvocation via an EXECUTE SERVICE command (triggered by a user\\ncommand, or through an orchestration framework), the service spins up on\\nthe compute pool, executes, and spins down. They are in some ways similar\\nto Snowflake tasks, with of course the exception that they execute on\\ncompute pools and give you the flexibility to run jobs that could not'],\n",
              " 'uris': None,\n",
              " 'data': None}"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#select with where\n",
        "collection.get(ids=[\"snw_1\"])"
      ],
      "metadata": {
        "id": "vrQ_D1h2XOrZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef25ca7b-e90b-4cff-cff4-f54b5274b414"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ids': [],\n",
              " 'embeddings': None,\n",
              " 'metadatas': [],\n",
              " 'documents': [],\n",
              " 'uris': None,\n",
              " 'data': None}"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "collection.query(query_texts=['object type']) #,where_document={\"$contains\":\"object type\"})"
      ],
      "metadata": {
        "id": "PWSAyvJcXtd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "collection.delete(ids= \"snw_1\")"
      ],
      "metadata": {
        "id": "ESkZr0L4a6mU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embeddings"
      ],
      "metadata": {
        "id": "kVsiBiyHYK_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "collection_em = client.create_collection(name = \"sample_collection_em\",metadata={\"hnsw:space\": \"cosine\"})\n",
        "client.list_collections()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tY3E0IQ3YOWy",
        "outputId": "29ecb297-67b4-40bf-fb98-3609a2351ec3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Collection(name=sample_collection_em), Collection(name=sample_collection)]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize embeddings\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
      ],
      "metadata": {
        "id": "zHzXKXf-cYal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_list = []\n",
        "documents_list = []\n",
        "ids_list = []\n",
        "for i, chunk in enumerate(chunks):\n",
        "    vector = embeddings.embed_query(chunk)\n",
        "    documents_list.append(chunk)\n",
        "    embeddings_list.append(vector)\n",
        "    ids_list.append(f\"snw_{i}\")"
      ],
      "metadata": {
        "id": "ON8nPk7eYiL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "collection_em.add(\n",
        "    embeddings=embeddings_list,\n",
        "    documents=documents_list,\n",
        "    ids=ids_list\n",
        ")"
      ],
      "metadata": {
        "id": "oycRaIIfYoHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "collection_em.get([],)"
      ],
      "metadata": {
        "id": "YBeRzLdNYxVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"object types\"\n",
        "query_vector = embeddings.embed_query(query)"
      ],
      "metadata": {
        "id": "EZmBTImTY69J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "collection_em.query(query_embeddings=query_vector)"
      ],
      "metadata": {
        "id": "pAh9otadY9jY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client.delete_collection(\"sample_collection\")"
      ],
      "metadata": {
        "id": "hGsrmI0DZinP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client.list_collections()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yl9AHkptZdhl",
        "outputId": "c53d2353-0e8d-441d-e796-0c811812f272"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Collection(name=sample_collection_em)]"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    }
  ]
}